name: PR Test Analysis Workflow

on:
  workflow_dispatch:
    inputs:
      repo_name:
        description: 'Repository name (format: owner/repo)'
        required: true
        type: string
      pr_number:
        description: 'Pull Request number'
        required: true
        type: string

env:
  JAVA_VERSION: '17'
  GRADLE_VERSION: '8.5'
  MAVEN_VERSION: '3.9.6'
  KOTLIN_VERSION: '1.9.0'
  PYTHON_VERSION: '3.10'

jobs:
  analyze-pr:
    runs-on: ubuntu-latest
    defaults:
      run:
        shell: bash
    permissions:
      contents: read
      pull-requests: read
      checks: write
      actions: read
      statuses: write
    
    steps:
      - name: Validate inputs
        run: |
          if [[ ! "${{ github.event.inputs.repo_name }}" =~ ^[A-Za-z0-9_.-]+/[A-Za-z0-9_.-]+$ ]]; then
            echo "‚ùå Invalid repository format. Expected: owner/repo"
            exit 1
          fi
          if [[ "${{ github.event.inputs.repo_name }}" =~ \.\. ]]; then
            echo "‚ùå Invalid repository name: path traversal detected"
            exit 1
          fi
          if [[ ! "${{ github.event.inputs.pr_number }}" =~ ^[0-9]+$ ]]; then
            echo "‚ùå Invalid PR number. Expected: numeric value"
            exit 1
          fi
          echo "‚úÖ Inputs validated successfully"

      - name: Setup Python
        uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Setup Java
        uses: actions/setup-java@v4
        with:
          distribution: 'temurin'
          java-version: ${{ env.JAVA_VERSION }}
          # Removed cache parameter to avoid errors

      - name: Setup Kotlin
        uses: fwilhe2/setup-kotlin@v1
        with:
          version: ${{ env.KOTLIN_VERSION }}

      - name: Cache dependencies
        id: cache-deps
        uses: actions/cache@v3
        with:
          path: |
            ~/.gradle/caches
            ~/.gradle/wrapper
            ~/.m2/repository
          key: ${{ runner.os }}-${{ hashFiles('**/build.gradle', '**/build.gradle.kts', '**/pom.xml') }}
          restore-keys: |
            ${{ runner.os }}-gradle-
            ${{ runner.os }}-maven-

      - name: Get PR information
        id: pr_info
        run: |
          PR_DATA=$(gh api repos/${{ github.event.inputs.repo_name }}/pulls/${{ github.event.inputs.pr_number }})
          BASE_SHA=$(echo "$PR_DATA" | jq -r '.base.sha')
          HEAD_SHA=$(echo "$PR_DATA" | jq -r '.head.sha')
          BASE_BRANCH=$(echo "$PR_DATA" | jq -r '.base.ref')
          echo "base_sha=$BASE_SHA" >> $GITHUB_OUTPUT
          echo "head_sha=$HEAD_SHA" >> $GITHUB_OUTPUT
          echo "base_branch=$BASE_BRANCH" >> $GITHUB_OUTPUT
        env:
          GH_TOKEN: ${{ secrets.GITHUB_TOKEN }}

      - name: Checkout base repository
        id: checkout-base
        uses: actions/checkout@v4
        with:
          repository: ${{ github.event.inputs.repo_name }}
          ref: ${{ steps.pr_info.outputs.base_branch || 'main' }}
          path: base-repo
          fetch-depth: 0
          token: ${{ secrets.GITHUB_TOKEN }}

      - name: Checkout PR repository
        id: checkout-pr
        uses: actions/checkout@v4
        with:
          repository: ${{ github.event.inputs.repo_name }}
          ref: refs/pull/${{ github.event.inputs.pr_number }}/merge
          path: pr-repo
          fetch-depth: 0
          token: ${{ secrets.GITHUB_TOKEN }}

      - name: Detect build system
        id: detect_build
        run: |
          if [ -f "pr-repo/pom.xml" ]; then
            echo "build_system=maven" >> $GITHUB_OUTPUT
            echo "üì¶ Detected Maven project"
          elif [ -f "pr-repo/build.gradle" ] || [ -f "pr-repo/build.gradle.kts" ]; then
            echo "build_system=gradle" >> $GITHUB_OUTPUT
            echo "üì¶ Detected Gradle project"
          else
            echo "‚ùå No supported build system found"
            exit 1
          fi

      - name: Setup Gradle
        if: steps.detect_build.outputs.build_system == 'gradle'
        uses: gradle/actions/setup-gradle@v3
        with:
          gradle-version: ${{ env.GRADLE_VERSION }}

      - name: Setup Maven
        if: steps.detect_build.outputs.build_system == 'maven'
        run: |
          wget -q https://dlcdn.apache.org/maven/maven-3/${{ env.MAVEN_VERSION }}/binaries/apache-maven-${{ env.MAVEN_VERSION }}-bin.tar.gz
          tar xzf apache-maven-${{ env.MAVEN_VERSION }}-bin.tar.gz
          echo "$PWD/apache-maven-${{ env.MAVEN_VERSION }}/bin" >> $GITHUB_PATH

      - name: Identify changed test files
        id: changed_tests
        run: |
          echo "üîç Identifying changed test files in PR..."
          cd pr-repo
          git diff --name-only ${{ steps.pr_info.outputs.base_sha }}..${{ steps.pr_info.outputs.head_sha }} | \
            grep -E '(Test|Spec)\.(java|kt)$|/(test|spec)/.*\.(java|kt)$' > ../changed_tests.txt || echo "No test files found" > ../changed_tests.txt
          
          if [ -s ../changed_tests.txt ] && grep -q "[^[:space:]]" ../changed_tests.txt; then
            echo "found_tests=true" >> $GITHUB_OUTPUT
            echo "üìã Changed test files:"
            cat ../changed_tests.txt
          else
            echo "found_tests=false" >> $GITHUB_OUTPUT
            echo "‚ö†Ô∏è No test files found in PR"
          fi

      - name: Compile base code
        id: compile_base
        if: steps.changed_tests.outputs.found_tests == 'true'
        run: |
          echo "üî® Compiling base code..."
          cd base-repo
          START_TIME=$(date +%s)
          
          if [ "${{ steps.detect_build.outputs.build_system }}" == "gradle" ]; then
            ./gradlew clean compileJava compileKotlin compileTestJava compileTestKotlin 2>&1 | tee ../compile_base.log
            COMPILE_STATUS=${PIPESTATUS[0]}
          else
            mvn clean compile test-compile 2>&1 | tee ../compile_base.log
            COMPILE_STATUS=${PIPESTATUS[0]}
          fi
          
          END_TIME=$(date +%s)
          DURATION=$((END_TIME - START_TIME))
          
          echo "compilation_status=$COMPILE_STATUS" >> $GITHUB_OUTPUT
          echo "compilation_time=$DURATION" >> $GITHUB_OUTPUT
          
          if [ $COMPILE_STATUS -eq 0 ]; then
            echo "‚úÖ Base compilation successful (${DURATION}s)"
          else
            echo "‚ùå Base compilation failed"
          fi

      - name: Copy PR tests to base
        if: steps.changed_tests.outputs.found_tests == 'true' && steps.compile_base.outputs.compilation_status == '0'
        run: |
          echo "üìã Copying PR test files to base code..."
          while IFS= read -r test_file; do
            if [ -n "$test_file" ]; then
              echo "Copying $test_file"
              mkdir -p "base-repo/$(dirname "$test_file")"
              cp "pr-repo/$test_file" "base-repo/$test_file"
            fi
          done < changed_tests.txt
          echo "‚úÖ Test files copied successfully"

      - name: Run tests with coverage
        id: run_tests
        if: steps.changed_tests.outputs.found_tests == 'true' && steps.compile_base.outputs.compilation_status == '0'
        timeout-minutes: 30
        run: |
          echo "üß™ Running tests with coverage..."
          cd base-repo
          START_TIME=$(date +%s)
          
          mkdir -p test-results
          
          # Extract test class names from file paths
          TEST_CLASSES=""
          while IFS= read -r test_file; do
            if [ -n "$test_file" ]; then
              # Convert file path to class name
              CLASS_NAME=$(echo "$test_file" | sed 's/src\/[^\/]*\/java\///' | sed 's/src\/[^\/]*\/kotlin\///' | sed 's/\.java$//' | sed 's/\.kt$//' | tr '/' '.')
              TEST_CLASSES="${TEST_CLASSES},${CLASS_NAME}"
            fi
          done < ../changed_tests.txt
          
          # Remove leading comma
          TEST_CLASSES=$(echo "$TEST_CLASSES" | sed 's/^,//')
          
          if [ "${{ steps.detect_build.outputs.build_system }}" == "gradle" ]; then
            # Use Gradle's built-in test filtering
            if [ -n "$TEST_CLASSES" ]; then
              ./gradlew test --tests "*${TEST_CLASSES//,/* --tests *}" jacocoTestReport 2>&1 | tee ../test_execution.log
            else
              ./gradlew test jacocoTestReport 2>&1 | tee ../test_execution.log
            fi
            TEST_STATUS=${PIPESTATUS[0]}
          else
            # Maven with specific test classes
            if [ -n "$TEST_CLASSES" ]; then
              mvn test -Dtest="$TEST_CLASSES" jacoco:report 2>&1 | tee ../test_execution.log
            else
              mvn test jacoco:report 2>&1 | tee ../test_execution.log
            fi
            TEST_STATUS=${PIPESTATUS[0]}
          fi
          
          END_TIME=$(date +%s)
          DURATION=$((END_TIME - START_TIME))
          
          echo "test_status=$TEST_STATUS" >> $GITHUB_OUTPUT
          echo "test_time=$DURATION" >> $GITHUB_OUTPUT
          
          # Extract test results
          if [ "${{ steps.detect_build.outputs.build_system }}" == "gradle" ]; then
            TESTS_RUN=$(grep -oP 'tests?: \K\d+' ../test_execution.log | tail -1 || echo "0")
            TESTS_PASSED=$(grep -oP 'passed?: \K\d+' ../test_execution.log | tail -1 || echo "0")
            TESTS_FAILED=$(grep -oP 'failed?: \K\d+' ../test_execution.log | tail -1 || echo "0")
            TESTS_SKIPPED=$(grep -oP 'skipped?: \K\d+' ../test_execution.log | tail -1 || echo "0")
          else
            TESTS_RUN=$(grep -oP 'Tests run: \K\d+' ../test_execution.log | tail -1 || echo "0")
            TESTS_FAILED=$(grep -oP 'Failures: \K\d+' ../test_execution.log | tail -1 || echo "0")
            TESTS_ERRORS=$(grep -oP 'Errors: \K\d+' ../test_execution.log | tail -1 || echo "0")
            TESTS_SKIPPED=$(grep -oP 'Skipped: \K\d+' ../test_execution.log | tail -1 || echo "0")
            TESTS_PASSED=$((TESTS_RUN - TESTS_FAILED - TESTS_ERRORS - TESTS_SKIPPED))
          fi
          
          echo "tests_run=$TESTS_RUN" >> $GITHUB_OUTPUT
          echo "tests_passed=$TESTS_PASSED" >> $GITHUB_OUTPUT
          echo "tests_failed=$TESTS_FAILED" >> $GITHUB_OUTPUT
          echo "tests_skipped=$TESTS_SKIPPED" >> $GITHUB_OUTPUT

      - name: Parse coverage results
        id: coverage
        if: steps.changed_tests.outputs.found_tests == 'true' && steps.run_tests.outputs.test_status == '0'
        run: |
          echo "üìä Parsing coverage results..."
          cd base-repo
          
          if [ "${{ steps.detect_build.outputs.build_system }}" == "gradle" ]; then
            COVERAGE_FILE="build/reports/jacoco/test/jacocoTestReport.xml"
          else
            COVERAGE_FILE="target/site/jacoco/jacoco.xml"
          fi
          
          if [ -f "$COVERAGE_FILE" ]; then
            # Parse JaCoCo XML for coverage metrics
            LINE_COVERAGE=$(python3 -c "
          import xml.etree.ElementTree as ET
          try:
              tree = ET.parse('$COVERAGE_FILE')
              root = tree.getroot()
              for counter in root.findall('.//counter[@type=\"LINE\"]'):
                  missed = int(counter.get('missed', 0))
                  covered = int(counter.get('covered', 0))
                  if (missed + covered) > 0:
                      print(f'{(covered / (missed + covered)) * 100:.2f}')
                      break
              else:
                  print('0.00')
          except Exception as e:
              print(f'Error parsing coverage: {e}')
              print('0.00')
          ")
            
            BRANCH_COVERAGE=$(python3 -c "
          import xml.etree.ElementTree as ET
          try:
              tree = ET.parse('$COVERAGE_FILE')
              root = tree.getroot()
              for counter in root.findall('.//counter[@type=\"BRANCH\"]'):
                  missed = int(counter.get('missed', 0))
                  covered = int(counter.get('covered', 0))
                  if (missed + covered) > 0:
                      print(f'{(covered / (missed + covered)) * 100:.2f}')
                      break
              else:
                  print('0.00')
          except Exception as e:
              print(f'Error parsing coverage: {e}')
              print('0.00')
          ")
            
            echo "line_coverage=$LINE_COVERAGE" >> $GITHUB_OUTPUT
            echo "branch_coverage=$BRANCH_COVERAGE" >> $GITHUB_OUTPUT
            echo "‚úÖ Coverage: Line=$LINE_COVERAGE%, Branch=$BRANCH_COVERAGE%"
          else
            echo "line_coverage=0.00" >> $GITHUB_OUTPUT
            echo "branch_coverage=0.00" >> $GITHUB_OUTPUT
            echo "‚ö†Ô∏è No coverage data found"
          fi

      - name: Compute metrics for summary
        id: compute_metrics
        if: always()
        run: |
          echo "üìà Computing metrics for summary report..."
          
          # Calculate success rate
          if [ "${{ steps.run_tests.outputs.tests_run }}" != "0" ] && [ -n "${{ steps.run_tests.outputs.tests_run }}" ]; then
            SUCCESS_RATE=$(echo "scale=2; ${{ steps.run_tests.outputs.tests_passed }} * 100 / ${{ steps.run_tests.outputs.tests_run }}" | bc)
          else
            SUCCESS_RATE="0"
          fi
          
          # Calculate total analysis time with defaults
          COMPILATION_TIME="${steps.compile_base.outputs.compilation_time:-0}"
          TEST_TIME="${steps.run_tests.outputs.test_time:-0}"
          TOTAL_TIME=$((COMPILATION_TIME + TEST_TIME))
          
          echo "success_rate=$SUCCESS_RATE" >> $GITHUB_OUTPUT
          echo "total_time=$TOTAL_TIME" >> $GITHUB_OUTPUT
          
          echo "‚úÖ Metrics computed: success_rate=$SUCCESS_RATE%, total_time=${TOTAL_TIME}s"

      - name: Generate summary report
        id: summary
        if: always()
        run: |
          echo "üìä Generating summary report..."
          
          # Set default values for variables that might be empty
          LINE_COVERAGE="${steps.coverage.outputs.line_coverage:-0.00}"
          BRANCH_COVERAGE="${steps.coverage.outputs.branch_coverage:-0.00}"
          MUTATION_SCORE="${steps.mutation.outputs.mutation_score:-0}"
          TOTAL_SMELLS="${steps.test_smells.outputs.total_smeels:-0}"
          
          cat > summary_report.md << EOF
          # PR Test Analysis Summary Report
          
          **Repository:** ${{ github.event.inputs.repo_name }}  
          **Pull Request:** #${{ github.event.inputs.pr_number }}  
          **Analysis Date:** $(date -u +"%Y-%m-%d %H:%M:%S UTC")  
          **Build System:** ${{ steps.detect_build.outputs.build_system }}
          
          ---
          
          ## üìã Overall Status
          
          | Metric | Status |
          |--------|--------|
          | Compilation | $([[ "${{ steps.compile_base.outputs.compilation_status }}" == "0" ]] && echo "‚úÖ Success" || echo "‚ùå Failed") |
          | Test Execution | $([[ "${{ steps.run_tests.outputs.test_status }}" == "0" ]] && echo "‚úÖ Success" || echo "‚ùå Failed") |
          | Changed Test Files | $([[ "${{ steps.changed_tests.outputs.found_tests }}" == "true" ]] && echo "$(wc -l < changed_tests.txt) files" || echo "None found") |
          
          ## üß™ Test Execution Results
          
          | Metric | Value |
          |--------|-------|
          | Total Tests Run | ${{ steps.run_tests.outputs.tests_run }} |
          | Tests Passed | ${{ steps.run_tests.outputs.tests_passed }} ‚úÖ |
          | Tests Failed | ${{ steps.run_tests.outputs.tests_failed }} ‚ùå |
          | Tests Skipped | ${{ steps.run_tests.outputs.tests_skipped }} ‚è≠Ô∏è |
          | Success Rate | ${{ steps.compute_metrics.outputs.success_rate }}% |
          | Execution Time | ${{ steps.run_tests.outputs.test_time }}s |
          
          ## üìä Code Coverage
          
          | Coverage Type | Percentage |
          |---------------|------------|
          | Line Coverage | ${LINE_COVERAGE}% |
          | Branch Coverage | ${BRANCH_COVERAGE}% |
          
          ## üß¨ Mutation Testing
          
          | Metric | Value |
          |--------|-------|
          | Mutation Score | ${MUTATION_SCORE}% |
          | Mutation Strength | $([[ "${MUTATION_SCORE}" -ge 80 ]] && echo "üü¢ Strong" || ([[ "${MUTATION_SCORE}" -ge 60 ]] && echo "üü° Moderate" || echo "üî¥ Weak")) |
          
          ## üîç Test Quality Analysis
          
          | Test Smell | Count |
          |------------|-------|
          EOF
          
          if [ -f "test_smells.json" ]; then
            python3 -c "
          import json
          try:
              with open('test_smells.json', 'r') as f:
                  data = json.load(f)
                  for smell, count in data.get('smells', {}).items():
                      icon = '‚ö†Ô∏è' if count > 0 else '‚úÖ'
                      print(f'| {smell.replace(\"_\", \" \").title()} | {count} {icon} |')
          except Exception as e:
              print('| Error loading smell data | - |')
          "
          else
            echo "| No analysis available | - |"
          fi >> summary_report.md
          
          cat >> summary_report.md << EOF
          | **Total Smells** | **${TOTAL_SMELLS}** |
          
          ## ‚è±Ô∏è Performance Metrics
          
          | Phase | Duration |
          |-------|----------|
          | Compilation | ${{ steps.compile_base.outputs.compilation_time }}s |
          | Test Execution | ${{ steps.run_tests.outputs.test_time }}s |
          | Total Analysis | ${{ steps.compute_metrics.outputs.total_time }}s |
          
          ## üìù Changed Test Files
          
          EOF
          
          if [ -f "changed_tests.txt" ] && [ -s "changed_tests.txt" ]; then
            echo '```' >> summary_report.md
            cat changed_tests.txt >> summary_report.md
            echo '```' >> summary_report.md
          else
            echo "_No test files were changed in this PR_" >> summary_report.md
          fi
          
          cat >> summary_report.md << EOF
          
          ## üéØ Quality Score
          
          EOF
          
          # Calculate overall quality score
          python3 -c "
          try:
              compilation = 1 if '${{ steps.compile_base.outputs.compilation_status }}' == '0' else 0
              test_success = float('${{ steps.run_tests.outputs.tests_passed }}') / max(float('${{ steps.run_tests.outputs.tests_run }}'), 1) if '${{ steps.run_tests.outputs.tests_run }}' != '0' else 0
              coverage = (float('${LINE_COVERAGE}') + float('${BRANCH_COVERAGE}')) / 200
              mutation = float('${MUTATION_SCORE}') / 100
              smells = max(0, 1 - (float('${TOTAL_SMELLS}') / 10))
              
              score = (compilation * 20 + test_success * 25 + coverage * 25 + mutation * 20 + smells * 10)
              
              grade = 'A' if score >= 90 else 'B' if score >= 80 else 'C' if score >= 70 else 'D' if score >= 60 else 'F'
              
              print(f'**Overall Quality Score:** {score:.1f}/100 (Grade: {grade})')
              print('')
              print('### Score Breakdown:')
              print(f'- Compilation: {compilation * 20:.1f}/20')
              print(f'- Test Success: {test_success * 25:.1f}/25')
              print(f'- Code Coverage: {coverage * 25:.1f}/25')
              print(f'- Mutation Testing: {mutation * 20:.1f}/20')
              print(f'- Code Quality: {smells * 10:.1f}/10')
          except Exception as e:
              print('Error calculating quality score: ' + str(e))
              print(f'**Overall Quality Score:** N/A')
          " >> summary_report.md
          
          echo "" >> summary_report.md
          echo "---" >> summary_report.md
          echo "_Generated by PR Test Analysis Workflow_" >> summary_report.md
          
          echo "‚úÖ Summary report generated successfully"

      - name: Upload test reports
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: test-analysis-reports
          path: |
            reports/
            summary_report.md
            test_smells.json
            compile_base.log
            test_execution.log
            mutation.log
          retention-days: 30

      - name: Upload coverage reports
        if: steps.run_tests.outputs.test_status == '0'
        uses: actions/upload-artifact@v4
        with:
          name: coverage-reports
          path: |
            base-repo/**/jacoco/**
            base-repo/**/site/jacoco/**
            base-repo/**/reports/jacoco/**
          retention-days: 30

      - name: Upload mutation reports
        if: steps.mutation.outputs.mutation_score != ''
        uses: actions/upload-artifact@v4
        with:
          name: mutation-reports
          path: |
            base-repo/**/pit-reports/**
            base-repo/**/reports/pitest/**
          retention-days: 30

      - name: Post summary as PR comment
        if: always() && steps.changed_tests.outputs.found_tests == 'true'
        uses: actions/github-script@v7
        with:
          github-token: ${{ secrets.GITHUB_TOKEN }}
          script: |
            const fs = require('fs');
            try {
              const summaryContent = fs.readFileSync('summary_report.md', 'utf8');
              await github.rest.issues.createComment({
                owner: '${{ github.event.inputs.repo_name }}'.split('/')[0],
                repo: '${{ github.event.inputs.repo_name }}'.split('/')[1],
                issue_number: ${{ github.event.inputs.pr_number }},
                body: summaryContent
              });
              console.log('‚úÖ Summary posted to PR successfully');
            } catch (error) {
              console.error('Failed to post comment:', error);
            }

      - name: Create check run
        if: always()
        uses: actions/github-script@v7
        with:
          github-token: ${{ secrets.GITHUB_TOKEN }}
          script: |
            const conclusion = '${{ steps.run_tests.outputs.test_status }}' === '0' ? 'success' : 'failure';
            const title = conclusion === 'success' ? '‚úÖ PR Test Analysis Passed' : '‚ùå PR Test Analysis Failed';
            
            const summary = `
            ## Test Results
            - **Tests Run:** ${{ steps.run_tests.outputs.tests_run }}
            - **Tests Passed:** ${{ steps.run_tests.outputs.tests_passed }}
            - **Tests Failed:** ${{ steps.run_tests.outputs.tests_failed }}
            - **Line Coverage:** ${{ steps.coverage.outputs.line_coverage }}%
            - **Branch Coverage:** ${{ steps.coverage.outputs.branch_coverage }}%
            - **Mutation Score:** ${{ steps.mutation.outputs.mutation_score }}%
            - **Test Smells:** ${{ steps.test_smells.outputs.total_smells }}
            `;
            
            try {
              await github.rest.checks.create({
                owner: '${{ github.event.inputs.repo_name }}'.split('/')[0],
                repo: '${{ github.event.inputs.repo_name }}'.split('/')[1],
                name: 'PR Test Analysis',
                head_sha: '${{ steps.pr_info.outputs.head_sha }}',
                status: 'completed',
                conclusion: conclusion,
                output: {
                  title: title,
                  summary: summary
                }
              });
            } catch (error) {
              console.error('Failed to create check run:', error);
            }

      - name: Cleanup
        if: always()
        run: |
          echo "üßπ Cleaning up temporary files..."
          rm -f changed_tests.txt compile_base.log test_execution.log mutation.log
          rm -f test_smells.json detect_smells.py
          echo "‚úÖ Cleanup completed"

      - name: Display final status
        if: always()
        run: |
          echo "========================================="
          echo "         PR TEST ANALYSIS COMPLETE      "
          echo "========================================="
          echo ""
          if [ "${{ steps.changed_tests.outputs.found_tests }}" != "true" ]; then
            echo "‚ö†Ô∏è  No test files found in PR #${{ github.event.inputs.pr_number }}"
          elif [ "${{ steps.compile_base.outputs.compilation_status }}" != "0" ]; then
            echo "‚ùå Analysis failed: Compilation error"
          elif [ "${{ steps.run_tests.outputs.test_status }}" != "0" ]; then
            echo "‚ùå Analysis completed with test failures"
            echo "   - Failed tests: ${{ steps.run_tests.outputs.tests_failed }}"
          else
            echo "‚úÖ Analysis completed successfully!"
            echo "   - Tests passed: ${{ steps.run_tests.outputs.tests_passed }}/${{ steps.run_tests.outputs.tests_run }}"
            echo "   - Coverage: ${{ steps.coverage.outputs.line_coverage }}%"
            echo "   - Mutation score: ${{ steps.mutation.outputs.mutation_score }}%"
          fi
          echo ""
          echo "üìä Full reports available in workflow artifacts"
          echo "========================================="

  # Separate job for generating detailed HTML report
  generate-html-report:
    needs: analyze-pr
    runs-on: ubuntu-latest
    if: always()
    
    steps:
      - name: Download artifacts
        uses: actions/download-artifact@v4
        with:
          path: artifacts
      
      - name: Generate HTML report
        run: |
          # Extract data from summary report
          LINE_COVERAGE=$(grep -oP 'Line Coverage.*\|\s*\K[0-9.]+' artifacts/summary_report.md 2>/dev/null || echo "0")
          BRANCH_COVERAGE=$(grep -oP 'Branch Coverage.*\|\s*\K[0-9.]+' artifacts/summary_report.md 2>/dev/null || echo "0")
          TESTS_PASSED=$(grep -oP 'Tests Passed.*\|\s*\K[0-9]+' artifacts/summary_report.md 2>/dev/null || echo "0")
          TESTS_RUN=$(grep -oP 'Total Tests Run.*\|\s*\K[0-9]+' artifacts/summary_report.md 2>/dev/null || echo "0")
          MUTATION_SCORE=$(grep -oP 'Mutation Score.*\|\s*\K[0-9.]+' artifacts/summary_report.md 2>/dev/null || echo "0")
          TOTAL_SMELLS=$(grep -oP 'Total Smells.*\|\s*\*\*[0-9]+\*\*' artifacts/summary_report.md | grep -oP '[0-9]+' 2>/dev/null || echo "0")
          
          # Calculate success percentage
          if [ "$TESTS_RUN" -gt 0 ]; then
            SUCCESS_PERCENTAGE=$((TESTS_PASSED * 100 / TESTS_RUN))
          else
            SUCCESS_PERCENTAGE=0
          fi
          
          cat > index.html << EOF
          <!DOCTYPE html>
          <html lang="en">
          <head>
              <meta charset="UTF-8">
              <meta name="viewport" content="width=device-width, initial-scale=1.0">
              <title>PR Test Analysis Report</title>
              <style>
                  * { margin: 0; padding: 0; box-sizing: border-box; }
                  body {
                      font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, sans-serif;
                      background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
                      min-height: 100vh;
                      padding: 20px;
                  }
                  .container {
                      max-width: 1200px;
                      margin: 0 auto;
                      background: white;
                      border-radius: 20px;
                      box-shadow: 0 20px 60px rgba(0,0,0,0.3);
                      overflow: hidden;
                  }
                  .header {
                      background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
                      color: white;
                      padding: 40px;
                      text-align: center;
                  }
                  .header h1 {
                      font-size: 2.5em;
                      margin-bottom: 10px;
                  }
                  .header p {
                      opacity: 0.9;
                      font-size: 1.1em;
                  }
                  .content {
                      padding: 40px;
                  }
                  .metrics-grid {
                      display: grid;
                      grid-template-columns: repeat(auto-fit, minmax(250px, 1fr));
                      gap: 20px;
                      margin-bottom: 40px;
                  }
                  .metric-card {
                      background: linear-gradient(135deg, #f5f7fa 0%, #c3cfe2 100%);
                      padding: 20px;
                      border-radius: 10px;
                      text-align: center;
                      transition: transform 0.3s;
                  }
                  .metric-card:hover {
                      transform: translateY(-5px);
                  }
                  .metric-value {
                      font-size: 2.5em;
                      font-weight: bold;
                      background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
                      -webkit-background-clip: text;
                      -webkit-text-fill-color: transparent;
                  }
                  .metric-label {
                      color: #666;
                      margin-top: 10px;
                  }
                  .section {
                      margin-bottom: 40px;
                  }
                  .section h2 {
                      color: #333;
                      border-bottom: 3px solid #667eea;
                      padding-bottom: 10px;
                      margin-bottom: 20px;
                  }
                  table {
                      width: 100%;
                      border-collapse: collapse;
                  }
                  th, td {
                      padding: 12px;
                      text-align: left;
                      border-bottom: 1px solid #eee;
                  }
                  th {
                      background: #f8f9fa;
                      font-weight: 600;
                      color: #667eea;
                  }
                  .status-badge {
                      display: inline-block;
                      padding: 4px 12px;
                      border-radius: 20px;
                      font-size: 0.9em;
                      font-weight: 600;
                  }
                  .status-success { background: #10b981; color: white; }
                  .status-failure { background: #ef4444; color: white; }
                  .status-warning { background: #f59e0b; color: white; }
                  .progress-bar {
                      width: 100%;
                      height: 30px;
                      background: #f3f4f6;
                      border-radius: 15px;
                      overflow: hidden;
                      position: relative;
                  }
                  .progress-fill {
                      height: 100%;
                      background: linear-gradient(90deg, #667eea 0%, #764ba2 100%);
                      transition: width 0.5s;
                      display: flex;
                      align-items: center;
                      justify-content: center;
                      color: white;
                      font-weight: bold;
                  }
                  .chart-container {
                      width: 100%;
                      height: 300px;
                      margin: 20px 0;
                  }
                  .footer {
                      background: #f8f9fa;
                      padding: 20px;
                      text-align: center;
                      color: #666;
                  }
              </style>
          </head>
          <body>
              <div class="container">
                  <div class="header">
                      <h1>üî¨ PR Test Analysis Report</h1>
                      <p>Comprehensive testing and quality analysis</p>
                  </div>
                  
                  <div class="content">
                      <div class="metrics-grid">
                          <div class="metric-card">
                              <div class="metric-value">$TESTS_PASSED/$TESTS_RUN</div>
                              <div class="metric-label">Tests Passed</div>
                          </div>
                          <div class="metric-card">
                              <div class="metric-value">${LINE_COVERAGE}%</div>
                              <div class="metric-label">Code Coverage</div>
                          </div>
                          <div class="metric-card">
                              <div class="metric-value">${MUTATION_SCORE}%</div>
                              <div class="metric-label">Mutation Score</div>
                          </div>
                          <div class="metric-card">
                              <div class="metric-value">$TOTAL_SMELLS</div>
                              <div class="metric-label">Test Smells</div>
                          </div>
                      </div>
                      
                      <div class="section">
                          <h2>üìä Test Results</h2>
                          <div class="progress-bar">
                              <div class="progress-fill" style="width: ${SUCCESS_PERCENTAGE}%">${SUCCESS_PERCENTAGE}%</div>
                          </div>
                      </div>
                      
                      <div class="section">
                          <h2>üìà Coverage Analysis</h2>
                          <table>
                              <thead>
                                  <tr>
                                      <th>Coverage Type</th>
                                      <th>Percentage</th>
                                      <th>Status</th>
                                  </tr>
                              </thead>
                              <tbody>
                                  <tr>
                                      <td>Line Coverage</td>
                                      <td>${LINE_COVERAGE}%</td>
                                      <td><span class="status-badge $([[ $(echo "$LINE_COVERAGE >= 80" | bc -l) -eq 1 ]] && echo "status-success" || ([[ $(echo "$LINE_COVERAGE >= 60" | bc -l) -eq 1 ]] && echo "status-warning" || echo "status-failure")) ">$([[ $(echo "$LINE_COVERAGE >= 80" | bc -l) -eq 1 ]] && echo "Good" || ([[ $(echo "$LINE_COVERAGE >= 60" | bc -l) -eq 1 ]] && echo "Fair" || echo "Poor"))</span></td>
                                  </tr>
                                  <tr>
                                      <td>Branch Coverage</td>
                                      <td>${BRANCH_COVERAGE}%</td>
                                      <td><span class="status-badge $([[ $(echo "$BRANCH_COVERAGE >= 80" | bc -l) -eq 1 ]] && echo "status-success" || ([[ $(echo "$BRANCH_COVERAGE >= 60" | bc -l) -eq 1 ]] && echo "status-warning" || echo "status-failure")) ">$([[ $(echo "$BRANCH_COVERAGE >= 80" | bc -l) -eq 1 ]] && echo "Good" || ([[ $(echo "$BRANCH_COVERAGE >= 60" | bc -l) -eq 1 ]] && echo "Fair" || echo "Poor"))</span></td>
                                  </tr>
                              </tbody>
                          </table>
                      </div>
                      
                      <div class="section">
                          <h2>üîç Code Quality</h2>
                          <table>
                              <thead>
                                  <tr>
                                      <th>Test Smell</th>
                                      <th>Count</th>
                                      <th>Severity</th>
                                  </tr>
                              </thead>
                              <tbody>
                                  <!-- Smells data would be populated here -->
                                  <tr>
                                      <td colspan="3" style="text-align: center;">Detailed smell analysis available in the test reports</td>
                                  </tr>
                              </tbody>
                          </table>
                      </div>
                  </div>
                  
                  <div class="footer">
                      <p>Generated by GitHub Actions Workflow ‚Ä¢ ¬© 2024</p>
                  </div>
              </div>
              
              <script>
                  // Animation for metrics
                  document.addEventListener('DOMContentLoaded', function() {
                      const metrics = document.querySelectorAll('.metric-value');
                      metrics.forEach(metric => {
                          metric.style.transition = 'all 0.5s ease-out';
                          metric.style.transform = 'scale(1.1)';
                          setTimeout(() => {
                              metric.style.transform = 'scale(1)';
                          }, 500);
                      });
                  });
              </script>
          </body>
          </html>
          EOF
          
          echo "‚úÖ HTML report generated"

      - name: Upload HTML report
        uses: actions/upload-artifact@v4
        with:
          name: html-report
          path: index.html
          retention-days: 30

      - name: Display workflow summary
        run: |
          echo "## üìã Workflow Execution Summary" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "| Component | Status |" >> $GITHUB_STEP_SUMMARY
          echo "|-----------|--------|" >> $GITHUB_STEP_SUMMARY
          echo "| PR Analysis | ‚úÖ Completed |" >> $GITHUB_STEP_SUMMARY
          echo "| Reports Generated | ‚úÖ Success |" >> $GITHUB_STEP_SUMMARY
          echo "| Artifacts Uploaded | ‚úÖ Success |" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "### üì¶ Available Artifacts:" >> $GITHUB_STEP_SUMMARY
          echo "- Test Analysis Reports" >> $GITHUB_STEP_SUMMARY
          echo "- Coverage Reports" >> $GITHUB_STEP_SUMMARY
          echo "- Mutation Reports" >> $GITHUB_STEP_SUMMARY
          echo "- HTML Summary Report" >> $GITHUB_STEP_SUMMARY

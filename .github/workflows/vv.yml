
name: Enhanced Test Metrics v4

on:
  workflow_dispatch:
    inputs:
      repo:
        description: "Repository (e.g. owner/name)"
        required: true
        type: string
      pr_number:
        description: "Pull request number"
        required: true
        type: string
      language:
        description: "Project language"
        required: true
        type: choice
        options:
          - python
          - cpp
          - java
          - kotlin
          - go
      timeout_minutes:
        description: "Workflow timeout in minutes"
        required: false
        type: number
        default: 45

env:
  MAX_RETRIES: 3
  RETRY_DELAY: 5
  MAX_DISK_USAGE_MB: 5000
  MAX_MEMORY_USAGE_MB: 3000

jobs:
  validate-inputs:
    runs-on: ubuntu-latest
    timeout-minutes: 2
    outputs:
      validated: ${{ steps.validate.outputs.validated }}
    steps:
      - name: Validate inputs
        id: validate
        run: |
          if [[ ! "${{ github.event.inputs.repo }}" =~ ^[a-zA-Z0-9_.-]+/[a-zA-Z0-9_.-]+$ ]]; then
            echo "❌ Invalid repository format: ${{ github.event.inputs.repo }}"
            exit 1
          fi
          
          if [[ ! "${{ github.event.inputs.pr_number }}" =~ ^[0-9]+$ ]]; then
            echo "❌ Invalid PR number: ${{ github.event.inputs.pr_number }}"
            exit 1
          fi
          
          timeout=${{ github.event.inputs.timeout_minutes }}
          if [[ $timeout -lt 5 || $timeout -gt 120 ]]; then
            echo "❌ Invalid timeout: $timeout minutes"
            exit 1
          fi
          
          echo "✅ All inputs validated successfully"
          echo "validated=true" >> $GITHUB_OUTPUT

  setup-environment:
    needs: validate-inputs
    runs-on: ubuntu-latest
    timeout-minutes: 10
    outputs:
      setup_complete: ${{ steps.setup.outputs.setup_complete }}
    steps:
      - name: Setup workspace and utilities
        id: setup
        run: |
          mkdir -p {logs,reports,artifacts,temp}
          
          # Create utility functions
          cat > utils.sh << 'EOF'
          #!/bin/bash
          
          retry() {
            local retries=$1
            shift
            local count=0
            until "$@"; do
              exit_code=$?
              count=$((count + 1))
              if [ $count -le $retries ]; then
                wait_time=$((2 ** (count - 1) * ${RETRY_DELAY:-5}))
                echo "⏳ Attempt $count failed, retrying in ${wait_time}s..."
                sleep $wait_time
              else
                echo "❌ All $retries attempts failed"
                return $exit_code
              fi
            done
            echo "✅ Command succeeded on attempt $count"
            return 0
          }
          
          check_disk_space() {
            local available=$(df . --output=avail -B1M | tail -1 | tr -d ' ')
            if [ $available -lt ${MAX_DISK_USAGE_MB:-5000} ]; then
              echo "❌ Insufficient disk space: ${available}MB available"
              return 1
            fi
            echo "✅ Disk space OK: ${available}MB available"
            return 0
          }
          
          log_info() {
            echo "$(date '+%Y-%m-%d %H:%M:%S') [INFO] $*"
          }
          
          log_warn() {
            echo "$(date '+%Y-%m-%d %H:%M:%S') [WARN] $*"
          }
          
          log_error() {
            echo "$(date '+%Y-%m-%d %H:%M:%S') [ERROR] $*"
          }
          
          cleanup_temp() {
            rm -rf temp/* 2>/dev/null || true
          }
          EOF
          
          chmod +x utils.sh
          echo "setup_complete=true" >> $GITHUB_OUTPUT

      - name: Upload utilities
        uses: actions/upload-artifact@v4
        with:
          name: workspace-utils
          path: |
            utils.sh
            logs/
            reports/
            artifacts/
            temp/
          retention-days: 1

  checkout-code:
    needs: [validate-inputs, setup-environment]
    runs-on: ubuntu-latest
    timeout-minutes: 10
    steps:
      - name: Download utilities
        uses: actions/download-artifact@v4
        with:
          name: workspace-utils

      - name: Checkout PR code
        run: |
          source utils.sh
          log_info "Checking out ${{ github.event.inputs.repo }}#${{ github.event.inputs.pr_number }}"
          
          retry $MAX_RETRIES git clone \
            --depth 1 --single-branch --no-tags \
            https://github.com/${{ github.event.inputs.repo }}.git repo_code
          
          cd repo_code
          retry $MAX_RETRIES git fetch origin pull/${{ github.event.inputs.pr_number }}/head:pr-branch
          git checkout pr-branch
          
          echo "✅ Checkout completed successfully"
          echo "Repository: $(git remote get-url origin)"
          echo "Branch: $(git branch --show-current)"
          echo "Commit: $(git rev-parse HEAD)"

      - name: Upload source code
        uses: actions/upload-artifact@v4
        with:
          name: source-code
          path: repo_code/
          retention-days: 1

  setup-caching:
    needs: [validate-inputs]
    runs-on: ubuntu-latest
    timeout-minutes: 5
    steps:
      - name: Setup comprehensive caching
        uses: actions/cache@v4
        with:
          path: |
            ~/.cache/pip
            ~/.m2/repository
            ~/.gradle/caches
            ~/.gradle/wrapper
            ~/.cache/go-build
            ~/go/pkg/mod
          key: ${{ runner.os }}-${{ github.event.inputs.language }}-${{ github.sha }}
          restore-keys: |
            ${{ runner.os }}-${{ github.event.inputs.language }}-
            ${{ runner.os }}-

  test-python:
    needs: [checkout-code, setup-caching]
    runs-on: ubuntu-latest
    timeout-minutes: ${{ fromJson(github.event.inputs.timeout_minutes) }}
    if: ${{ github.event.inputs.language == 'python' }}
    steps:
      - name: Download artifacts
        uses: actions/download-artifact@v4
        with:
          pattern: "*"

      - name: Setup Python environment
        run: |
          source utils.sh
          log_info "Setting up Python environment"
          
          # Move source code
          cp -r source-code/* . 2>/dev/null || true
          
          # Setup Python
          retry $MAX_RETRIES sudo apt-get update
          retry $MAX_RETRIES sudo apt-get install -y python3.11 python3.11-venv python3-pip
          
          python3.11 -m venv venv
          source venv/bin/activate
          
          retry $MAX_RETRIES pip install --upgrade pip wheel setuptools

      - name: Install Python dependencies
        run: |
          source utils.sh
          source venv/bin/activate
          
          log_info "Installing Python dependencies"
          
          DEPS_INSTALLED=false
          for dep_file in "requirements.txt" "requirements/requirements.txt" "pyproject.toml" "setup.py"; do
            if [ -f "$dep_file" ]; then
              log_info "Found dependency file: $dep_file"
              case "$dep_file" in
                "requirements.txt"|"requirements/"*)
                  if retry $MAX_RETRIES pip install -r "$dep_file"; then
                    DEPS_INSTALLED=true
                    break
                  fi
                  ;;
                "pyproject.toml"|"setup.py")
                  if retry $MAX_RETRIES pip install -e .; then
                    DEPS_INSTALLED=true
                    break
                  fi
                  ;;
              esac
            fi
          done
          
          # Install testing tools
          retry $MAX_RETRIES pip install pytest pytest-cov coverage mutmut lxml flake8

      - name: Run Python tests
        run: |
          source utils.sh
          source venv/bin/activate
          set +e
          
          log_info "Running Python tests with coverage"
          
          # Find test directories
          TEST_DIRS=()
          for dir in tests test testing; do
            if [ -d "$dir" ]; then
              TEST_DIRS+=("$dir")
            fi
          done
          
          if [ ${#TEST_DIRS[@]} -eq 0 ]; then
            TEST_FILES=$(find . -name "*test*.py" -o -name "test_*.py" | head -10)
            if [ -n "$TEST_FILES" ]; then
              TEST_DIRS=(".")
            fi
          fi
          
          if [ ${#TEST_DIRS[@]} -eq 0 ]; then
            echo "NO_TESTS_FOUND" > logs/test_status.txt
            log_warn "No tests found"
          else
            timeout 1800 pytest "${TEST_DIRS[@]}" \
              --cov=. --cov-report=xml:reports/coverage.xml \
              --cov-report=term-missing --tb=short -v \
              > logs/pytest.log 2>&1
            
            TEST_EXIT_CODE=$?
            if [ $TEST_EXIT_CODE -eq 0 ]; then
              echo "TESTS_PASSED" > logs/test_status.txt
              log_info "✅ Tests passed"
              
              # Run mutation testing
              timeout 3600 mutmut run --runner "pytest ${TEST_DIRS[0]} -x -q" \
                --paths-to-mutate . --simple-output \
                > logs/mutmut.log 2>&1 || echo "MUTATION_FAILED" > logs/mutation_status.txt
            else
              echo "TESTS_FAILED" > logs/test_status.txt
              log_warn "⚠️ Some tests failed"
            fi
            
            # Detect test smells
            flake8 --select=PT --exit-zero "${TEST_DIRS[@]}" > reports/test_smells.txt 2>&1 || true
          fi

      - name: Upload Python results
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: python-results
          path: |
            logs/
            reports/
          retention-days: 30

  test-cpp:
    needs: [checkout-code, setup-caching]
    runs-on: ubuntu-latest
    timeout-minutes: ${{ fromJson(github.event.inputs.timeout_minutes) }}
    if: ${{ github.event.inputs.language == 'cpp' }}
    steps:
      - name: Download artifacts
        uses: actions/download-artifact@v4
        with:
          pattern: "*"

      - name: Setup C++ environment
        run: |
          source utils.sh
          cp -r source-code/* . 2>/dev/null || true
          
          log_info "Setting up C++ environment"
          retry $MAX_RETRIES sudo apt-get update
          retry $MAX_RETRIES sudo apt-get install -y \
            build-essential cmake ninja-build \
            lcov gcovr clang llvm \
            libgtest-dev libgmock-dev

      - name: Build and test C++
        run: |
          source utils.sh
          set +e
          
          mkdir -p build && cd build
          
          # Configure with coverage
          cmake .. \
            -DCMAKE_BUILD_TYPE=Debug \
            -DCMAKE_CXX_FLAGS="--coverage -g -O0" \
            -DCMAKE_EXE_LINKER_FLAGS="--coverage" \
            > ../logs/cmake.log 2>&1
          
          CMAKE_EXIT_CODE=$?
          if [ $CMAKE_EXIT_CODE -ne 0 ]; then
            echo "BUILD_FAILED" > ../logs/build_status.txt
            exit 0
          fi
          
          # Build
          make -j$(nproc) > ../logs/build.log 2>&1
          BUILD_EXIT_CODE=$?
          
          if [ $BUILD_EXIT_CODE -eq 0 ]; then
            # Run tests
            if command -v ctest &> /dev/null; then
              ctest --output-on-failure -V > ../logs/ctest.log 2>&1
              TEST_EXIT_CODE=$?
              if [ $TEST_EXIT_CODE -eq 0 ]; then
                echo "TESTS_PASSED" > ../logs/test_status.txt
              else
                echo "TESTS_FAILED" > ../logs/test_status.txt
              fi
            fi
            
            # Generate coverage
            cd ..
            gcovr -r . --xml -o reports/coverage.xml \
              --print-summary > logs/coverage.log 2>&1 || true
          else
            echo "BUILD_FAILED" > ../logs/build_status.txt
          fi

      - name: Upload C++ results
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: cpp-results
          path: |
            logs/
            reports/
          retention-days: 30

  test-java:
    needs: [checkout-code, setup-caching]
    runs-on: ubuntu-latest
    timeout-minutes: ${{ fromJson(github.event.inputs.timeout_minutes) }}
    if: ${{ github.event.inputs.language == 'java' }}
    steps:
      - name: Setup JDK
        uses: actions/setup-java@v4
        with:
          java-version: '17'
          distribution: 'temurin'

      - name: Download artifacts
        uses: actions/download-artifact@v4
        with:
          pattern: "*"

      - name: Build and test Java
        run: |
          source utils.sh
          cp -r source-code/* . 2>/dev/null || true
          set +e
          
          if [ -f "pom.xml" ]; then
            log_info "Building with Maven"
            
            timeout 1800 mvn clean compile -B > logs/maven_compile.log 2>&1
            COMPILE_EXIT_CODE=$?
            
            if [ $COMPILE_EXIT_CODE -eq 0 ]; then
              timeout 2400 mvn test jacoco:report -B > logs/maven_test.log 2>&1
              TEST_EXIT_CODE=$?
              
              if [ $TEST_EXIT_CODE -eq 0 ]; then
                echo "TESTS_PASSED" > logs/test_status.txt
                
                # Run PIT mutation testing
                timeout 3600 mvn org.pitest:pitest-maven:mutationCoverage \
                  -DoutputFormats=XML > logs/pitest.log 2>&1 || true
              else
                echo "TESTS_FAILED" > logs/test_status.txt
              fi
            else
              echo "BUILD_FAILED" > logs/build_status.txt
            fi
            
          elif [ -f "build.gradle" ] || [ -f "build.gradle.kts" ]; then
            log_info "Building with Gradle"
            [ -f "gradlew" ] && chmod +x gradlew
            GRADLE_CMD=$([ -f "gradlew" ] && echo "./gradlew" || echo "gradle")
            
            timeout 1800 $GRADLE_CMD clean build > logs/gradle_build.log 2>&1
            BUILD_EXIT_CODE=$?
            
            if [ $BUILD_EXIT_CODE -eq 0 ]; then
              timeout 2400 $GRADLE_CMD test jacocoTestReport > logs/gradle_test.log 2>&1
              TEST_EXIT_CODE=$?
              
              if [ $TEST_EXIT_CODE -eq 0 ]; then
                echo "TESTS_PASSED" > logs/test_status.txt
              else
                echo "TESTS_FAILED" > logs/test_status.txt
              fi
            else
              echo "BUILD_FAILED" > logs/build_status.txt
            fi
          else
            echo "BUILD_FAILED" > logs/build_status.txt
            log_error "No recognized Java build system found"
          fi

      - name: Upload Java results
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: java-results
          path: |
            logs/
            reports/
          retention-days: 30

  test-kotlin:
    needs: [checkout-code, setup-caching]
    runs-on: ubuntu-latest
    timeout-minutes: ${{ fromJson(github.event.inputs.timeout_minutes) }}
    if: ${{ github.event.inputs.language == 'kotlin' }}
    steps:
      - name: Setup JDK
        uses: actions/setup-java@v4
        with:
          java-version: '17'
          distribution: 'temurin'

      - name: Download artifacts
        uses: actions/download-artifact@v4
        with:
          pattern: "*"

      - name: Build and test Kotlin
        run: |
          source utils.sh
          cp -r source-code/* . 2>/dev/null || true
          set +e
          
          [ -f "gradlew" ] && chmod +x gradlew
          GRADLE_CMD=$([ -f "gradlew" ] && echo "./gradlew" || echo "gradle")
          
          log_info "Building Kotlin project"
          timeout 1800 $GRADLE_CMD clean build > logs/kotlin_build.log 2>&1
          BUILD_EXIT_CODE=$?
          
          if [ $BUILD_EXIT_CODE -eq 0 ]; then
            timeout 2400 $GRADLE_CMD test > logs/kotlin_test.log 2>&1
            TEST_EXIT_CODE=$?
            
            if [ $TEST_EXIT_CODE -eq 0 ]; then
              echo "TESTS_PASSED" > logs/test_status.txt
              
              # Generate coverage with Kover
              $GRADLE_CMD koverXmlReport > logs/kover.log 2>&1 || true
            else
              echo "TESTS_FAILED" > logs/test_status.txt
            fi
          else
            echo "BUILD_FAILED" > logs/build_status.txt
          fi

      - name: Upload Kotlin results
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: kotlin-results
          path: |
            logs/
            reports/
          retention-days: 30

  test-go:
    needs: [checkout-code, setup-caching]
    runs-on: ubuntu-latest
    timeout-minutes: ${{ fromJson(github.event.inputs.timeout_minutes) }}
    if: ${{ github.event.inputs.language == 'go' }}
    steps:
      - name: Setup Go
        uses: actions/setup-go@v5
        with:
          go-version: '1.21'

      - name: Download artifacts
        uses: actions/download-artifact@v4
        with:
          pattern: "*"

      - name: Build and test Go
        run: |
          source utils.sh
          cp -r source-code/* . 2>/dev/null || true
          set +e
          
          # Initialize module if needed
          [ ! -f "go.mod" ] && go mod init temp-module 2>/dev/null || true
          
          log_info "Downloading Go dependencies"
          retry $MAX_RETRIES go mod download
          go mod tidy 2>/dev/null || true
          
          # Build
          timeout 1200 go build -v ./... > logs/go_build.log 2>&1
          BUILD_EXIT_CODE=$?
          
          if [ $BUILD_EXIT_CODE -eq 0 ]; then
            # Run tests with coverage
            timeout 2400 go test -v -race -coverprofile=reports/coverage.out \
              -covermode=atomic ./... > logs/go_test.log 2>&1
            TEST_EXIT_CODE=$?
            
            if [ $TEST_EXIT_CODE -eq 0 ]; then
              echo "TESTS_PASSED" > logs/test_status.txt
              
              # Generate coverage reports
              if [ -f "reports/coverage.out" ]; then
                go tool cover -func=reports/coverage.out > reports/coverage.txt 2>&1 || true
                
                # Convert to XML if tool is available
                if go install github.com/t-yuki/gocover-cobertura@latest 2>/dev/null; then
                  gocover-cobertura < reports/coverage.out > reports/coverage.xml 2>&1 || true
                fi
              fi
              
              # Simple test smell detection
              TEST_SMELLS=0
              TODO_COUNT=$(grep -r "TODO\|FIXME" --include="*_test.go" . 2>/dev/null | wc -l || echo 0)
              TEST_SMELLS=$((TEST_SMELLS + TODO_COUNT))
              echo "$TEST_SMELLS" > reports/test_smells.txt
            else
              echo "TESTS_FAILED" > logs/test_status.txt
            fi
          else
            echo "BUILD_FAILED" > logs/build_status.txt
          fi

      - name: Upload Go results
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: go-results
          path: |
            logs/
            reports/
          retention-days: 30

  parse-metrics:
    needs: [test-python, test-cpp, test-java, test-kotlin, test-go]
    if: always()
    runs-on: ubuntu-latest
    timeout-minutes: 10
    steps:
      - name: Download all results
        uses: actions/download-artifact@v4
        with:
          pattern: "*-results"

      - name: Download utilities
        uses: actions/download-artifact@v4
        with:
          name: workspace-utils

      - name: Parse and analyze metrics
        run: |
          source utils.sh
          
          # Merge all results
          mkdir -p {logs,reports}
          find . -name "logs" -type d -exec cp -r {}/* logs/ \; 2>/dev/null || true
          find . -name "reports" -type d -exec cp -r {}/* reports/ \; 2>/dev/null || true
          
          log_info "Starting metrics parsing"
          
          python3 << 'EOF'
          import json
          import os
          import xml.etree.ElementTree as ET
          import re
          from datetime import datetime

          # Initialize metrics
          metrics = {
              "repo": "${{ github.event.inputs.repo }}",
              "pr_number": "${{ github.event.inputs.pr_number }}",
              "language": "${{ github.event.inputs.language }}",
              "coverage": None,
              "mutation_score": None,
              "compilation_success": True,
              "execution_success": True,
              "test_smells": 0,
              "timestamp": datetime.now().isoformat()
          }

          # Check build/test status
          if os.path.exists("logs/build_status.txt"):
              with open("logs/build_status.txt") as f:
                  if "BUILD_FAILED" in f.read():
                      metrics["compilation_success"] = False

          if os.path.exists("logs/test_status.txt"):
              with open("logs/test_status.txt") as f:
                  content = f.read()
                  if "TESTS_FAILED" in content:
                      metrics["execution_success"] = False
                  elif "NO_TESTS_FOUND" in content:
                      metrics["execution_success"] = None

          # Parse coverage
          coverage_files = ["reports/coverage.xml", "coverage.xml"]
          for cov_file in coverage_files:
              if os.path.exists(cov_file):
                  try:
                      tree = ET.parse(cov_file)
                      root = tree.getroot()
                      
                      # Try Cobertura format
                      if 'line-rate' in root.attrib:
                          metrics["coverage"] = round(float(root.attrib['line-rate']) * 100, 2)
                          break
                      
                      # Try JaCoCo format
                      for counter in tree.findall(".//counter[@type='LINE']"):
                          if 'covered' in counter.attrib and 'missed' in counter.attrib:
                              covered = int(counter.attrib['covered'])
                              missed = int(counter.attrib['missed'])
                              total = covered + missed
                              if total > 0:
                                  metrics["coverage"] = round((covered * 100.0) / total, 2)
                                  break
                      
                      if metrics["coverage"] is not None:
                          break
                  except Exception as e:
                      print(f"Error parsing coverage file {cov_file}: {e}")

          # Parse Go text coverage
          if os.path.exists("reports/coverage.txt"):
              try:
                  with open("reports/coverage.txt") as f:
                      content = f.read()
                      match = re.search(r'total:.*?(\d+\.\d+)%', content)
                      if match:
                          metrics["coverage"] = round(float(match.group(1)), 2)
              except Exception as e:
                  print(f"Error parsing Go coverage: {e}")

          # Parse test smells
          if os.path.exists("reports/test_smells.txt"):
              try:
                  with open("reports/test_smells.txt") as f:
                      content = f.read().strip()
                      if content:
                          try:
                              metrics["test_smells"] = int(content.split('\n')[0])
                          except ValueError:
                              metrics["test_smells"] = len([line for line in content.split('\n') if line.strip()])
              except Exception as e:
                  print(f"Error parsing test smells: {e}")

          # Generate summary
          coverage_status = "N/A"
          if metrics["coverage"] is not None:
              if metrics["coverage"] >= 80:
                  coverage_status = "🟢 Excellent"
              elif metrics["coverage"] >= 60:
                  coverage_status = "🟡 Good"
              else:
                  coverage_status = "🔴 Needs Improvement"

          summary = f"""# 📊 Test Metrics Report

          ## 📋 Repository Information
          - **Repository**: `{metrics['repo']}`
          - **Pull Request**: #{metrics['pr_number']}
          - **Language**: {metrics['language'].title()}

          ## 🏗️ Build Status
          - **Compilation**: {'✅ Success' if metrics['compilation_success'] else '❌ Failed'}
          - **Test Execution**: {'✅ Success' if metrics['execution_success'] else '❌ Failed' if metrics['execution_success'] is False else '⚠️ No Tests Found'}

          ## 📈 Code Quality Metrics
          - **Code Coverage**: {f"{metrics['coverage']}%" if metrics['coverage'] is not None else 'N/A'} ({coverage_status})
          - **Test Smells**: {metrics['test_smells']}

          ## 🎯 Recommendations
          """

          recommendations = []
          if not metrics["compilation_success"]:
              recommendations.append("🔧 **Fix compilation errors** - Code must compile successfully")
          if metrics["execution_success"] is False:
              recommendations.append("🧪 **Fix failing tests** - Address test failures")
          if metrics["coverage"] is not None and metrics["coverage"] < 60:
              recommendations.append("📊 **Improve test coverage** - Aim for at least 60% coverage")
          if metrics["test_smells"] > 10:
              recommendations.append("🧹 **Reduce test smells** - Refactor test code")
          
          if not recommendations:
              recommendations.append("🎉 **Great job!** - Your code quality metrics look good")

          for i, rec in enumerate(recommendations, 1):
              summary += f"{i}. {rec}\n"

          # Save results
          os.makedirs("final_reports", exist_ok=True)
          
          with open("final_reports/metrics.json", "w") as f:
              json.dump(metrics, f, indent=2)
          
          with open("final_reports/summary.md", "w") as f:
              f.write(summary)

          print("=== FINAL METRICS ===")
          print(json.dumps(metrics, indent=2))
          print("=" * 50)

          # Exit with appropriate code
          if not metrics["compilation_success"]:
              exit(1)
          elif metrics["execution_success"] is False:
              exit(2)
          else:
              exit(0)
          EOF

      - name: Upload final metrics
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: final-metrics-${{ github.event.inputs.language }}-${{ github.event.inputs.pr_number }}
          path: |
            final_reports/
            logs/
            reports/
          retention-days: 90

      - name: Generate job summary
        if: always()
        run: |
          echo "# 🚀 Test Metrics Workflow Results" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "**Repository:** \`${{ github.event.inputs.repo }}\`" >> $GITHUB_STEP_SUMMARY
          echo "**PR Number:** #${{ github.event.inputs.pr_number }}" >> $GITHUB_STEP_SUMMARY
          echo "**Language:** ${{ github.event.inputs.language }}" >> $GITHUB_STEP_SUMMARY
          echo "**Workflow Run:** ${{ github.run_number }}" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          
          if [ -f "final_reports/summary.md" ]; then
            cat final_reports/summary.md >> $GITHUB_STEP_SUMMARY
          else
            echo "⚠️ **Metrics summary not available**" >> $GITHUB_STEP_SUMMARY
            echo "" >> $GITHUB_STEP_SUMMARY
            echo "The workflow may have encountered issues during execution." >> $GITHUB_STEP_SUMMARY
            echo "Please check the individual job logs for more details:" >> $GITHUB_STEP_SUMMARY
            echo "" >> $GITHUB_STEP_SUMMARY
            echo "- 🔍 [Setup Jobs](https://github.com/${{ github.repository }}/actions/runs/${{ github.run_id }})" >> $GITHUB_STEP_SUMMARY
            echo "- 🧪 [Test Execution](https://github.com/${{ github.repository }}/actions/runs/${{ github.run_id }})" >> $GITHUB_STEP_SUMMARY
            echo "- 📊 [Metrics Parsing](https://github.com/${{ github.repository }}/actions/runs/${{ github.run_id }})" >> $GITHUB_STEP_SUMMARY
          fi
          
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "## 🔗 Quick Links" >> $GITHUB_STEP_SUMMARY
          echo "- 📊 [Download All Artifacts](https://github.com/${{ github.repository }}/actions/runs/${{ github.run_id }})" >> $GITHUB_STEP_SUMMARY
          echo "- 🔍 [View Pull Request](https://github.com/${{ github.event.inputs.repo }}/pull/${{ github.event.inputs.pr_number }})" >> $GITHUB_STEP_SUMMARY
          echo "- 📋 [Workflow Source](https://github.com/${{ github.repository }}/blob/main/.github/workflows/)" >> $GITHUB_STEP_SUMMARY
